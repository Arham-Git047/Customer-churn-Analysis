# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wVGxrInlWwkUz5XLOvkTPy48og-LseQQ
"""

# Install necessary libraries
!pip install imbalanced-learn scikit-learn xgboost seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set(style="whitegrid")

# Load dataset
df = pd.read_csv("/Telco_Customer_Churn_Dataset  (3).csv")

# Initial inspection
print(f"Dataset Shape: {df.shape}")
print("\nData Types:")
print(df.dtypes)
print("\nMissing Values:")
print(df.isnull().sum())

# Check for duplicate customer IDs
print(f"\nDuplicate Customers: {df['customerID'].duplicated().sum()}")

# Display sample
df.head()

# Handle TotalCharges conversion
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Drop missing values (11 rows)
df.dropna(inplace=True)

# Convert SeniorCitizen to categorical
df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'})

# Remove unnecessary column
df.drop('customerID', axis=1, inplace=True)

# Handle "No internet service" and "No phone service"
internet_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                 'TechSupport', 'StreamingTV', 'StreamingMovies']

for col in internet_cols:
    df[col] = df[col].replace('No internet service', 'No')

df['MultipleLines'] = df['MultipleLines'].replace('No phone service', 'No')

# 1. Target Variable Distribution
plt.figure(figsize=(6,4))
sns.countplot(x='Churn', data=df)
plt.title('Churn Distribution')
plt.show()

# 2. Numerical Features Analysis
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
fig, ax = plt.subplots(1, 3, figsize=(18,5))
for i, col in enumerate(num_cols):
    sns.histplot(df[col], kde=True, ax=ax[i])
    ax[i].set_title(f'{col} Distribution')
plt.tight_layout()
plt.show()

# 3. Categorical Features Analysis
cat_cols = [col for col in df.columns if col not in num_cols + ['Churn']]

for col in cat_cols:
    plt.figure(figsize=(10,5))
    sns.countplot(x=col, hue='Churn', data=df)
    plt.title(f'Churn Distribution by {col}')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# 4. Correlation Analysis
corr_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Create tenure groups
df['TenureGroup'] = pd.cut(df['tenure'], bins=[0, 12, 24, 48, 72],
                          labels=['0-1yr', '1-2yr', '2-4yr', '4-6yr'])

# Create spending ratio
df['SpendingRatio'] = df['MonthlyCharges'] / df['TotalCharges']

# Create service count
services = ['PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',
            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']

df['ServiceCount'] = df[services].apply(lambda x: (x == 'Yes').sum(), axis=1)

# Define features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Preprocessing pipeline
numeric_features = num_cols + ['SpendingRatio', 'ServiceCount']
categorical_features = cat_cols + ['TenureGroup']

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Apply SMOTE for class imbalance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(
    preprocessor.fit_transform(X_train), y_train
)

# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Define features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Encode the target variable 'Churn' to numerical (0 and 1)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y) # Use the encoded y

# Split data using the encoded y
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded # Stratify on the encoded y
)

# Preprocessing pipeline
numeric_features = num_cols + ['SpendingRatio', 'ServiceCount']
categorical_features = cat_cols + ['TenureGroup']

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Apply SMOTE for class imbalance using the encoded y_train
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(
    preprocessor.fit_transform(X_train), y_train
)

# Preprocess test set outside the loop (convert to dense array)
# X_test_preprocessed = preprocessor.transform(X_test).toarray()  # Convert to dense array - Removed .toarray()
X_test_preprocessed = preprocessor.transform(X_test)

# Train and evaluate models
results = {}

# Define the models dictionary - This was missing in the original code execution
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

for name, model in models.items():
    model.fit(X_train_res, y_train_res)
    y_pred = model.predict(X_test_preprocessed)

    # Handle models without predict_proba (shouldn't occur with these models)
    if hasattr(model, "predict_proba"):
        # predict_proba returns probabilities for all classes, we need the probability of the positive class (which is now 1)
        y_proba = model.predict_proba(X_test_preprocessed)[:, 1]
        roc_auc = roc_auc_score(y_test, y_proba)
    else:
        y_proba = None
        roc_auc = None

    # Store results
    results[name] = {
        "Classification Report": classification_report(y_test, y_pred),
        "Confusion Matrix": confusion_matrix(y_test, y_pred),
        "ROC AUC": roc_auc
    }

    # Print results
    print(f"\n{name} Results:")
    print(classification_report(y_test, y_pred))
    if roc_auc is not None:
        print(f"ROC AUC: {roc_auc:.4f}")

    # Plot ROC curve if probabilities are available
    if y_proba is not None:
        # Now y_test is 0 and 1, so pos_label is implicitly handled
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot all ROC curves
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison')
plt.legend()
plt.show()

# Get feature names
cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']
cat_features = cat_encoder.get_feature_names_out(categorical_features)
all_features = numeric_features + list(cat_features)

# Plot feature importance for Random Forest
rf = models["Random Forest"]
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1][:20]

plt.figure(figsize=(12,8))
plt.title("Top 20 Feature Importances (Random Forest)")
plt.barh(range(len(indices)), importances[indices][::-1], align="center")
plt.yticks(range(len(indices)), [all_features[i] for i in indices][::-1])
plt.xlabel("Relative Importance")
plt.show()

# Plot feature importance for XGBoost
xgb = models["XGBoost"]
importances = xgb.feature_importances_
indices = np.argsort(importances)[::-1][:20]

plt.figure(figsize=(12,8))
plt.title("Top 20 Feature Importances (XGBoost)")
plt.barh(range(len(indices)), importances[indices][::-1], align="center")
plt.yticks(range(len(indices)), [all_features[i] for i in indices][::-1])
plt.xlabel("Relative Importance")
plt.show()

# Generate final report summary
report = """
=== CHURN ANALYSIS REPORT ===
Dataset Size: {} customers
Churn Rate: {:.1f}%

Key Drivers:
1. Contract Type:
   - Month-to-month churn: {:.1f}%
   - 2-year contract churn: {:.1f}%

2. Internet Service:
   - Fiber optic churn: {:.1f}%
   - DSL churn: {:.1f}%

3. Payment Method:
   - Electronic check churn: {:.1f}%
   - Automatic payments churn: {:.1f}%

Model Performance:
- Best Model: {} (AUC: {:.2f})
- Churn Detection Rate: {:.1f}%

Recommendations:
1. Convert month-to-month to contracts with incentives
2. Bundle security services with fiber packages
3. Offer 5-10% discount for auto-pay enrollment
4. Implement 90-day onboarding success program
5. Develop predictive churn dashboard
""".format(
    len(df),
    df['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['Contract']=='Month-to-month']['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['Contract']=='Two year']['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['InternetService']=='Fiber optic']['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['InternetService']=='DSL']['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['PaymentMethod']=='Electronic check']['Churn'].value_counts(normalize=True)['Yes']*100,
    df[df['PaymentMethod']=='Bank transfer (automatic)']['Churn'].value_counts(normalize=True)['Yes']*100,
    "XGBoost",
    results["XGBoost"]["ROC AUC"],
    # Convert the extracted string to float before formatting
    float(results["XGBoost"]["Classification Report"].split('\n')[3].split()[3])*100 # Multiply by 100 for percentage
)

print(report)